import pandas as pd
import numpy as np
import os
import scipy.stats as stats
from statsmodels.stats.multitest import multipletests
from scipy.stats import entropy, iqr, skew, kurtosis
from scipy.signal import find_peaks

root_path = "/mnt/zzbnew/peixunban/changan/try2/AS_result"
save_path = "/mnt/zzbnew/peixunban/changan/try2/plot_resulets/"

os.makedirs(save_path, exist_ok=True)

# ========== å®šä¹‰è¾…åŠ©å‡½æ•° ==========
def get_percent_mean(x, percent=0.05, top=True):
    """è·å–å‰/å percent ç™¾åˆ†æ¯”æ•°æ®çš„å‡å€¼"""
    x_clean = x.dropna()
    if len(x_clean) == 0:
        return np.nan
    n = max(1, int(len(x_clean) * percent))
    if top:
        return x_clean.nlargest(n).mean()
    else:
        return x_clean.nsmallest(n).mean()

def analyze_peaks(x):
    """åˆ†æå³°å€¼ï¼šå³°å€¼æ•°é‡ã€å³°å€¼å¯†åº¦ã€å³°å€¼å‡å€¼"""
    x_clean = x.dropna().values
    if len(x_clean) < 3:
        return pd.Series([0, 0, np.nan])
    
    # æ‰¾å³°å€¼ï¼ˆé«˜åº¦è‡³å°‘ä¸ºå‡å€¼ï¼‰
    peaks, _ = find_peaks(x_clean, height=x_clean.mean())
    peak_count = len(peaks)
    peak_density = peak_count / len(x_clean) if len(x_clean) > 0 else 0
    peak_mean = x_clean[peaks].mean() if peak_count > 0 else np.nan
    
    return pd.Series([peak_count, peak_density, peak_mean])

def calc_entropy(x):
    """è®¡ç®—é¦™å†œç†µ"""
    x_clean = x[x > 0].dropna()
    if len(x_clean) == 0:
        return 0
    probs = x_clean / x_clean.sum()
    return entropy(probs)

# è·å–åŸºå› æ–‡ä»¶å¤¹
genes = sorted([
    d for d in os.listdir(root_path)
    if os.path.isdir(os.path.join(root_path, d)) and d != "1.plot"
])
print("æ‰¾åˆ°åŸºå› æ–‡ä»¶å¤¹ï¼š", genes)

results = []

for gene in genes:
    gene_dir = os.path.join(root_path, gene)
    file_hap1 = os.path.join(gene_dir, "hap1_attention_collapsed.csv")#å®šä¹‰ä½¿ç”¨åŸºå› çš„hap1_attention_collapsed.csvæ–‡ä»¶
    file_meta = os.path.join(gene_dir, "metadata.csv")

    if not (os.path.exists(file_hap1) and os.path.exists(file_meta)):
        print(f"âš ï¸ è·³è¿‡ {gene}ï¼ˆç¼ºå°‘ CSV æ–‡ä»¶ï¼‰")
        continue

    # è¯»å–æ•°æ®
    df = pd.read_csv(file_hap1)
    metadata = pd.read_csv(file_meta)

    # ========== è®¡ç®—æ‰€æœ‰ç»Ÿè®¡æŒ‡æ ‡ ==========
    value_cols = df.columns[1:]
    data_values = df[value_cols]
    
    result = pd.DataFrame({"sample": df["sample"]})
    
    # åŸºç¡€ç»Ÿè®¡é‡
    result["mean"] = data_values.mean(axis=1)
    result["max"] = data_values.max(axis=1)
    result["std"] = data_values.std(axis=1)
    result["cv"] = result["std"] / result["mean"]
    result["median"] = data_values.median(axis=1)
    result["iqr"] = data_values.apply(lambda x: iqr(x, nan_policy='omit'), axis=1)
    result["percentile_90"] = data_values.quantile(0.9, axis=1)
    result["percentile_10"] = data_values.quantile(0.1, axis=1)
    
    # ä¼—æ•°
    result["mode"] = data_values.apply(
        lambda x: stats.mode(x.dropna(), keepdims=True, nan_policy='omit')[0][0] 
        if not x.dropna().empty else np.nan, axis=1
    )
    
    # ååº¦ä¸å³°åº¦
    result["skewness"] = data_values.apply(lambda x: skew(x, nan_policy='omit'), axis=1)
    result["kurtosis"] = data_values.apply(lambda x: kurtosis(x, nan_policy='omit'), axis=1)
    
    # è‡ªå®šä¹‰å¤æ‚æŒ‡æ ‡
    result["top5_percent_mean"] = data_values.apply(lambda x: get_percent_mean(x, 0.05, True), axis=1)
    result["low5_percent_mean"] = data_values.apply(lambda x: get_percent_mean(x, 0.05, False), axis=1)
    result[["peak_count", "peak_density", "peak_mean"]] = data_values.apply(analyze_peaks, axis=1)
    result["shannon_entropy"] = data_values.apply(calc_entropy, axis=1)

    # åˆå¹¶ metadata å¹¶æ˜ å°„åˆ†ç»„
    result = result.merge(metadata, on="sample", how="left")
    result["group"] = result["sample_type"].map({0: "control", 2: "case"})

    # ========== å¯¹æ‰€æœ‰æŒ‡æ ‡åš U æ£€éªŒ ==========
    metrics = [
        "mean", "max", "std", "cv", "median", "mode", "iqr", "skewness", "kurtosis",
        "top5_percent_mean", "low5_percent_mean", "percentile_90", "percentile_10",
        "peak_count", "peak_density", "peak_mean", "shannon_entropy"
    ]
    
    p_values = [gene]

    for metric in metrics:
        case_data = result[result["group"] == "case"][metric].dropna()
        ctrl_data = result[result["group"] == "control"][metric].dropna()
        if len(case_data) > 0 and len(ctrl_data) > 0:
            _, p_val = stats.mannwhitneyu(case_data, ctrl_data, alternative='two-sided')
        else:
            p_val = np.nan
        p_values.append(p_val)

    results.append(p_values)
    print(f"âœ“ å®Œæˆ {gene}")

# ========== ç”Ÿæˆç»“æœè¡¨ ==========
columns = ["gene"] + [f"{m}_p" for m in metrics]
df_out = pd.DataFrame(results, columns=columns)

# ========== BH correctionï¼ˆå¤šé‡æ£€éªŒæ ¡æ­£ï¼‰==========
print("\nå¼€å§‹è¿›è¡Œ BH correction...")
for metric in metrics:
    p_col = f"{metric}_p"
    # è¿‡æ»¤æ‰ NaN å€¼
    valid_mask = ~df_out[p_col].isna()
    valid_pvals = df_out.loc[valid_mask, p_col]
    
    if len(valid_pvals) > 0:
        # è¿›è¡Œ BH correction
        rejected, corrected_pvals, _, _ = multipletests(
            valid_pvals, 
            alpha=0.05, 
            method='fdr_bh'
        )
        # åˆ›å»ºæ–°åˆ—å­˜å‚¨æ ¡æ­£åçš„ p å€¼
        df_out.loc[valid_mask, f"{metric}_p_corrected"] = corrected_pvals
        df_out.loc[valid_mask, f"{metric}_significant"] = rejected
    else:
        df_out[f"{metric}_p_corrected"] = np.nan
        df_out[f"{metric}_significant"] = False

# æŒ‰ std_p æ’åº
df_out = df_out.sort_values("std_p")

# ä¿å­˜ç»“æœ
out_file = os.path.join(save_path, "17stats_pvalues_with_BH_correction.csv") #ä¿å­˜ç»“æœæ–‡ä»¶è·¯å¾„
df_out.to_csv(out_file, index=False)
print(f"\nğŸ‰ ç»“æœå·²ä¿å­˜è‡³ï¼š{out_file}")

# ========== è¾“å‡ºç»Ÿè®¡æ‘˜è¦ ==========
print("\n" + "="*80)
print("ç»Ÿè®¡æ‘˜è¦")
print("="*80)
print(f"æ€»åŸºå› æ•°: {len(df_out)}")

for metric in metrics:
    p_col = f"{metric}_p"
    p_corr_col = f"{metric}_p_corrected"
    sig_col = f"{metric}_significant"
    
    # åŸå§‹ p < 0.05 çš„åŸºå› æ•°
    uncorrected_sig = df_out[df_out[p_col] < 0.05].shape[0]
    # BH æ ¡æ­£åæ˜¾è‘—çš„åŸºå› æ•°
    corrected_sig = df_out[df_out[sig_col] == True].shape[0]
    
    print(f"\n{metric.upper()}:")
    print(f"  åŸå§‹ P < 0.05: {uncorrected_sig} ä¸ªåŸºå› ")
    print(f"  BH æ ¡æ­£åæ˜¾è‘—: {corrected_sig} ä¸ªåŸºå› ")

# ========== è¾“å‡ºæ˜¾è‘—åŸºå› åˆ—è¡¨ ==========
print("\n" + "="*80)
print("BH æ ¡æ­£åæ˜¾è‘—åŸºå› åˆ—è¡¨")
print("="*80)

for metric in metrics:
    sig_col = f"{metric}_significant"
    p_col = f"{metric}_p"
    p_corr_col = f"{metric}_p_corrected"
    
    sig_genes = df_out[df_out[sig_col] == True].sort_values(p_corr_col)
    
    if not sig_genes.empty:
        print(f"\nğŸ”¹ {metric.upper()} æ˜¾è‘—åŸºå›  ({len(sig_genes)} ä¸ª):")
        for _, row in sig_genes.iterrows():
            print(f"  {row['gene']:<20}  åŸå§‹p = {row[p_col]:.2e}  æ ¡æ­£p = {row[p_corr_col]:.2e}")
    else:
        print(f"\nğŸ”¹ {metric.upper()}: æ— æ˜¾è‘—åŸºå› ")

print("\n" + "="*80)
print("å‰10ä¸ªåŸºå› ï¼ˆæŒ‰ std_p æ’åºï¼‰ï¼š")
print("="*80)
print(df_out.head(10).to_string(index=False))